{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5062a4c2",
   "metadata": {},
   "source": [
    "# Image and text Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de56e1",
   "metadata": {},
   "source": [
    "## Image Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc068ccb",
   "metadata": {},
   "source": [
    "1. Embeddings\n",
    "2. Decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691dcb6",
   "metadata": {},
   "source": [
    "### 1. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a871727",
   "metadata": {},
   "source": [
    "- Transfer learning = pretrained models on large dataset and apply that knowledge to smaller dataset.\n",
    "- ResNet50 Model: That was trained on ImageNet dataset.\n",
    "- For feature extraction use use the last hidden layers as numerical representation.\n",
    "- There are two probelms with this approach. \n",
    "    -  The transfer learning (RestNet50) trained on the model only on images found in ImageNet.  But if there are differenttypes of images, diagrames, book pages, drawing, satellite image  the embeddings learned from the imagenet dataset may not works so well.\n",
    "    - Also the labels may not be useful for specific classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba05ba",
   "metadata": {},
   "source": [
    "### Auxiliary Learning Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3046f3a",
   "metadata": {},
   "source": [
    "- Another way to create embedding.\n",
    "- A model is trained to predict the next work in sentence. \n",
    "- Auto encoders take advantage of an auxiliary learning task for images. \n",
    "- Similar to predict the next word model.\n",
    "\n",
    "** Autoencoders**\n",
    "- We take the image data, Pass it through a network that bottlenecks it into samllar internel vector.\n",
    "- Then expands it back out into the dimensionality of the original image. \n",
    "- Any information in the inputs that isnt useful gets dropped.\n",
    "- Input image ---> Encode --- latent dimension ---> Decoder ---> Reconstructed image.\n",
    "- Ideal dimension size is tuned with expermiments. (Balanced not to lost important information and not to take too much noise)\n",
    "- While restore the shape during decoding, The Conv2DTranspose - copies the value to its surrounding pixels.\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b8fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder\n",
    "encoder = tf.keras.Sequential([\n",
    "    keras.Input(Shape=(28,28,1), name=\"image_input\")\n",
    "    layers.Conv2D(32, 3, activation=\"relu\", strider=2, padding=\"same\"),\n",
    "    layers.Conv2d(64, 3, activation=\"relu\", strides=2,  padding=\"same\"),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(2) ] , name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decoder \n",
    "decoder = tf.kerasa.Sequential([\n",
    "    keras.Input(shape=(latent_dim,), name =\"d_input\"),\n",
    "    layers.Dense(7 * 7 * 64, activation=\"relu\"),\n",
    "    layers.Reshape((7, 7, 64)),\n",
    "    layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", strides=2, padding=\"same\")\n",
    "    ], name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training \n",
    "encoder_inputs = keras.Input(shape=(28,28,1))\n",
    "x = encoder(encoder_inputs)\n",
    "decoder_output = decoder(X)\n",
    "autoencoder = keras.Model(encoder_inputs , decoder_output)\n",
    "\n",
    "### Mean squeare error is caluclated between input and output images to calculate the loss function.\n",
    "autoencoder.complie(optimizer=keras.optimizers.Adam(), loss='mse')\n",
    "history = autoencoder.fit(mnist_digits, mnist_digits, epochs=30, batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05370f68",
   "metadata": {},
   "source": [
    "Once the mode is trained, we can drop the decoder and use the ecoder to covnert images into latent vectors.\n",
    "z = encoder.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f85c5c",
   "metadata": {},
   "source": [
    "**Variational autoencoders (VAEs)** were developed in response to classic autoencoders being unable to use their decoders to generate quality images from user-generated latent vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bca055",
   "metadata": {},
   "source": [
    "- VAE are generative models.\n",
    "Generative models, on the other hand, learn a joint probability distribution that explicitly models the distribution of each class. The conditional probability distribution isn’t lost—we can still do classification using Bayes’ theorem, for example.\n",
    "\n",
    "Fortunately, most of the architecture of variational autoencoders is the same as that of classic autoencoders: the hourglass shape, reconstruction loss, etc. However, the few additional complexities allow VAEs to do what autoencoders can’t: image generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5619393",
   "metadata": {},
   "source": [
    "**Implementing VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten(name=\"e_flatten\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()(z_mean, z_log_var)\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "### Sampling layer in TensofFlow\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs)\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(input=z_mean)[0]\n",
    "        dim = tf.shape(input=z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.math.exp(x=0.5 * z_log_var) * epsilon \n",
    "    \n",
    "\n",
    "### VAE\n",
    "z_mean, z_log_var, z = encoder(encoder_input)\n",
    "decoder_ouput = decoder(z)\n",
    "vae = keras.Model(encoder_inputs, decoder_output, name=\"vae\")\n",
    "\n",
    "### LOSS\n",
    "# we cannot use the MSE\n",
    "# KUllback-Liebler divargence - \n",
    "\n",
    "def kl_divergence(z_mean, z_log_var):\n",
    "    kl_loss = 0.5 * (1 + z_log_var \n",
    "                     \n",
    "\n",
    "\n",
    "history = vae.fit(mnist_digits, mnist_digits, epochs=30,\n",
    "                  batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8fc6f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
